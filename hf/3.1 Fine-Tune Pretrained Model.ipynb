{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec97152",
   "metadata": {},
   "source": [
    "# Fine Tuning a Pretrained Model\n",
    "* Previous Chapter: Use Tokenizers & Pretrained Models to make predictions\n",
    "* This Chapter: Fine-tune a pretrained model for custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "695e3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51bce917",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Model + Tokenize Sequences\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\"\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b376ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing model using two examples we gave\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae7415a",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18f87f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/n/fs/nlp-jy1682/hf_datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0e60c34ed248979e4bfb564f558f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Dataset Description:\n",
       "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DatasetDict</span><span style=\"font-weight: bold\">({</span>\n",
       "    train: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'sentence1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sentence2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'idx'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3668</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    validation: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'sentence1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sentence2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'idx'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">408</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "    test: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Dataset</span><span style=\"font-weight: bold\">({</span>\n",
       "        features: <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'sentence1'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'sentence2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'idx'</span><span style=\"font-weight: bold\">]</span>,\n",
       "        num_rows: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1725</span>\n",
       "    <span style=\"font-weight: bold\">})</span>\n",
       "<span style=\"font-weight: bold\">})</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Dataset Description:\n",
       "\u001b[1;35mDatasetDict\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "    train: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'sentence1'\u001b[0m, \u001b[32m'sentence2'\u001b[0m, \u001b[32m'label'\u001b[0m, \u001b[32m'idx'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m3668\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    validation: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'sentence1'\u001b[0m, \u001b[32m'sentence2'\u001b[0m, \u001b[32m'label'\u001b[0m, \u001b[32m'idx'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m408\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "    test: \u001b[1;35mDataset\u001b[0m\u001b[1m(\u001b[0m\u001b[1m{\u001b[0m\n",
       "        features: \u001b[1m[\u001b[0m\u001b[32m'sentence1'\u001b[0m, \u001b[32m'sentence2'\u001b[0m, \u001b[32m'label'\u001b[0m, \u001b[32m'idx'\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        num_rows: \u001b[1;36m1725\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training Example:\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sentence1'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">evidence .'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sentence2'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">his evidence .'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'idx'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training Example:\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'sentence1'\u001b[0m: \u001b[32m'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his \u001b[0m\n",
       "\u001b[32mevidence .'\u001b[0m,\n",
       "    \u001b[32m'sentence2'\u001b[0m: \u001b[32m'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting \u001b[0m\n",
       "\u001b[32mhis evidence .'\u001b[0m,\n",
       "    \u001b[32m'label'\u001b[0m: \u001b[1;36m1\u001b[0m,\n",
       "    \u001b[32m'idx'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Training Features:\n",
       "<span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sentence1'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sentence2'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'string'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'label'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ClassLabel</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">names</span>=<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'not_equivalent'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'equivalent'</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'idx'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Value</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'int32'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">id</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Training Features:\n",
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'sentence1'\u001b[0m: \u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'sentence2'\u001b[0m: \u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'string'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'label'\u001b[0m: \u001b[1;35mClassLabel\u001b[0m\u001b[1m(\u001b[0m\u001b[33mnames\u001b[0m=\u001b[1m[\u001b[0m\u001b[32m'not_equivalent'\u001b[0m, \u001b[32m'equivalent'\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'idx'\u001b[0m: \u001b[1;35mValue\u001b[0m\u001b[1m(\u001b[0m\u001b[33mdtype\u001b[0m=\u001b[32m'int32'\u001b[0m, \u001b[33mid\u001b[0m=\u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "print(\"Dataset Description:\", raw_datasets)\n",
    "print(\"Training Example:\", raw_datasets[\"train\"][0])\n",
    "print(\"Training Features:\", raw_datasets[\"train\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff7057",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e0db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize strings from DatasetDict object\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a4af23",
   "metadata": {},
   "source": [
    "Quick revisit of tokens -> ids, ids -> tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cea83d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">101</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1996</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2034</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6251</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2023</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2003</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1996</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2117</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2028</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1012</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">102</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'token_type_ids'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"font-weight: bold\">[</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'input_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m101\u001b[0m, \u001b[1;36m2023\u001b[0m, \u001b[1;36m2003\u001b[0m, \u001b[1;36m1996\u001b[0m, \u001b[1;36m2034\u001b[0m, \u001b[1;36m6251\u001b[0m, \u001b[1;36m1012\u001b[0m, \u001b[1;36m102\u001b[0m, \u001b[1;36m2023\u001b[0m, \u001b[1;36m2003\u001b[0m, \u001b[1;36m1996\u001b[0m, \u001b[1;36m2117\u001b[0m, \u001b[1;36m2028\u001b[0m, \u001b[1;36m1012\u001b[0m, \u001b[1;36m102\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'token_type_ids'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m0\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'attention_mask'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize two sequences as a pair\n",
    "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
    "print(inputs)\n",
    "# input_ids: integers that each uniquely identify a token in the sentence\n",
    "# attention_mask: (0/1) Whether the token should be accounted for by the tokenizer\n",
    "# token_type_ids: number corresponds to which sentence (only available for some models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "518d6d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'[CLS]'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'first'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'sentence'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'this'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'is'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'the'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'second'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'one'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'.'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'[SEP]'</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'this'\u001b[0m,\n",
       "    \u001b[32m'is'\u001b[0m,\n",
       "    \u001b[32m'the'\u001b[0m,\n",
       "    \u001b[32m'first'\u001b[0m,\n",
       "    \u001b[32m'sentence'\u001b[0m,\n",
       "    \u001b[32m'.'\u001b[0m,\n",
       "    \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m,\n",
       "    \u001b[32m'this'\u001b[0m,\n",
       "    \u001b[32m'is'\u001b[0m,\n",
       "    \u001b[32m'the'\u001b[0m,\n",
       "    \u001b[32m'second'\u001b[0m,\n",
       "    \u001b[32m'one'\u001b[0m,\n",
       "    \u001b[32m'.'\u001b[0m,\n",
       "    \u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m'\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Decode IDs back to words (has special tokens added)\n",
    "print(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06f9867",
   "metadata": {},
   "source": [
    "Now, let's tokenize the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e02008c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Dataset Object Type: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'datasets.dataset_dict.DatasetDict'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Dataset Object Type: \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'datasets.dataset_dict.DatasetDict'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Dataset Keys: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">dict_keys</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'validation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Dataset Keys: \u001b[1;35mdict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'train'\u001b[0m, \u001b[32m'validation'\u001b[0m, \u001b[32m'test'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - Approach 1 -\n",
    "tokenized_datasets = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True)\n",
    "print(\"Dataset Object Type:\", type(tokenized_dataset))\n",
    "print(\"Dataset Keys:\", tokenized_dataset.keys())\n",
    "# Disadvantages: Returns dictionary, requires storing whole dataset in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0a0cf32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /n/fs/nlp-jy1682/hf_datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b5b0894350f13edc.arrow\n",
      "Loading cached processed dataset at /n/fs/nlp-jy1682/hf_datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-604d48d6b86f2095.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9215c7d1df72483cb37eed35b7b0569a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - Approach 2 -\n",
    "def tokenize_function(example):\n",
    "    # Dict (i.e. dataset item) => Dict (input_ids, attention_mask, token_type_ids)\n",
    "    # Also works if `example` dict contains several samples\n",
    "    # `padding` excluded because it may take a lot of time\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "# Apply tokenization function on dataset\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc693069",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Dataset Object Type: <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'datasets.dataset_dict.DatasetDict'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Dataset Object Type: \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'datasets.dataset_dict.DatasetDict'\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Dataset Keys: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">dict_keys</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008000; text-decoration-color: #008000\">'train'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'validation'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'test'</span><span style=\"font-weight: bold\">])</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Dataset Keys: \u001b[1;35mdict_keys\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'train'\u001b[0m, \u001b[32m'validation'\u001b[0m, \u001b[32m'test'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Dataset Object Type:\", type(tokenized_dataset))\n",
    "print(\"Dataset Keys:\", tokenized_dataset.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e792e80d",
   "metadata": {},
   "source": [
    "**Dynamic Padding**: Solution to pad all examples to length of longest element when batching elements together (`collate` function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eb0b26b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c67228c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try out `data_collator` on 8 samples.\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19c6bc1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'input_ids'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span><span style=\"font-weight: bold\">])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'token_type_ids'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span><span style=\"font-weight: bold\">])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'attention_mask'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">67</span><span style=\"font-weight: bold\">])</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'labels'</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">torch.Size</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">])</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'input_ids'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m67\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'token_type_ids'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m67\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'attention_mask'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m67\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'labels'\u001b[0m: \u001b[1;35mtorch.Size\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now dynamically pad with collator\n",
    "batch = data_collator(samples)\n",
    "print({k: v.shape for k, v in batch.items()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
